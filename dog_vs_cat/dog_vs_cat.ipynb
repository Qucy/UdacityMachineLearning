{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理\n",
    "- 原始数据集直接从Kaggle上下载，解压并且经过预处理之后分别放在 train/cat，train/dog以及test/none目录下（详见readme）\n",
    "- 同时项目中提供了一个额外的[扩充数据](http://www.robots.ox.ac.uk/%7Evgg/data/pets/)，扩充数据集总共有7390张图片，猫的图片有2400张，狗的图片有4990张\n",
    "- 数据解压后文件全部在images文件夹下，文件名的格式为{种类}_{序号}.jpg，比如 Abyssinian_1.jpg\n",
    "- 同时官方给出了所有图片的种类明细，比如Abyssinian是猫。所以应该把图片Abyssinian_1.jpg分类到train/cat的目录下面\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图片目录结构\n",
    "当前的图片目录结构如下\n",
    "```\n",
    "data\n",
    " ├── images [7390 images]\n",
    " ├── test\n",
    " │   └── none [12500 images]\n",
    " └── train\n",
    "     ├── cat [12500 images]\n",
    "     └── dog [12500 images]\n",
    "```\n",
    "我们需要把images下的7390张图片，根据官方给出的类别分类到train目录的cat和dog子目录中。分类成功的话新的数据集猫的图片数量应该为12500+2400=14900，狗的图片数量为12500+4990=17490。处理后的图片目录结构应该如下\n",
    "```\n",
    "data\n",
    " ├── test\n",
    " │   └── none [12500 images]\n",
    " └── train\n",
    "     ├── cat [14900 images]\n",
    "     └── dog [17490 images]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cat test images:  2400\n",
      "Local dog test images:  4990\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "dog_breeds = ['american_bulldog', 'american_pit_bull_terrier','basset_hound','beagle','boxer',\n",
    "             'chihuahua','english_cocker_spaniel','english_setter','german_shorthaired','great_pyrenees',\n",
    "             'havanese','japanese_chin','keeshond','leonberger','miniature_pinscher','newfoundland','pomeranian',\n",
    "             'pug','saint_bernard','samoyed','scottish_terrier','shiba_inu','staffordshire_bull_terrier',\n",
    "             'wheaten_terrier','yorkshire_terrier']\n",
    "\n",
    "cat_breeds = ['Abyssinian','Bengal','Birman','Bombay','British_Shorthair','Egyptian_Mau','Maine_Coon','Persian',\n",
    "              'Ragdoll','Russian_Blue','Siamese','Sphynx']\n",
    "\n",
    "src = 'data/images/'\n",
    "dog_dest = 'data/local_test/dog/'\n",
    "cat_dest = 'data/local_test/cat/'\n",
    "\n",
    "os.makedirs(dog_dest, exist_ok=True)\n",
    "os.makedirs(cat_dest, exist_ok=True)\n",
    "\n",
    "def copy_extra_data(dest, breeds):\n",
    "    for root, dirs, files in os.walk(src):\n",
    "        for name in files:\n",
    "            for val in breeds:\n",
    "                if(name.find(val)>-1):\n",
    "                    copyfile(src + name, dest + name)\n",
    "                    \n",
    "\n",
    "copy_extra_data(dog_dest, dog_breeds)\n",
    "copy_extra_data(cat_dest, cat_breeds)\n",
    "\n",
    "print(\"Local cat test images: \", \n",
    "      len([name for name in os.listdir(cat_dest) if os.path.isfile(os.path.join(cat_dest, name))]))\n",
    "print(\"Local dog test images: \", \n",
    "      len([name for name in os.listdir(dog_dest) if os.path.isfile(os.path.join(dog_dest, name))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导出深度特征\n",
    "- 根据当前训练以及测试集导出VGG16,VGG19,ResNet50,Xception以及InceptionV3的深度特征\n",
    "- VGG16,VGG19,ResNet50要求的图片的大小为（224， 224）\n",
    "- Xception，Inception要求的图片大小为（299，299）\n",
    "- 先对所有数据进行一个预处理的操作，把数据缩放到-1到1之间\n",
    "- 其次我们加入一个平局池化操作，一方面是缩小我们导出的深度特征文件的大小，另一方是防止过拟合\n",
    "- 最后使用Keras的ImageGenerator对数据进行增加处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "import time\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "train_data_path = 'data/train/'\n",
    "test_data_path = 'data/test/'\n",
    "local_test_data_path = 'data/local_test/'\n",
    "\n",
    "def save_bottleneck_features(MODEL, image_size, module_name, preprocess):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    input_tensor = Input((height, width, 3))\n",
    "    x = Lambda(preprocess)(input_tensor)\n",
    "    \n",
    "    base_model = MODEL(input_tensor=x, weights='imagenet', include_top=False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "\n",
    "    gen = ImageDataGenerator()\n",
    "    train_generator = gen.flow_from_directory(train_data_path, image_size, shuffle=False)\n",
    "    test_generator = gen.flow_from_directory(test_data_path, image_size, shuffle=False, class_mode=None)\n",
    "    local_test_generator = gen.flow_from_directory(local_test_data_path, image_size, shuffle=False, class_mode=None)\n",
    "\n",
    "    train = model.predict_generator(train_generator)\n",
    "    test = model.predict_generator(test_generator)\n",
    "    local_test = model.predict_generator(local_test_generator)\n",
    "    \n",
    "    with h5py.File(\"bottleneck_features/{}_bottleneck_features.h5\".format(module_name)) as h:\n",
    "        h.create_dataset(\"train\", data=train)\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "        h.create_dataset(\"label\", data=train_generator.classes)\n",
    "        h.create_dataset(\"local_test\", data=local_test)\n",
    "        h.create_dataset(\"local_test_label\", data=local_test_generator.classes)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"{} extrac features total consumed: {} seconds\".format(module_name, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "Found 7390 images belonging to 2 classes.\n",
      "VGG16 extrac features total consumed: 268.376017332077 seconds\n"
     ]
    }
   ],
   "source": [
    "save_bottleneck_features(VGG16, (224, 224), 'VGG16', vgg16.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "Found 7390 images belonging to 2 classes.\n",
      "VGG19 extrac features total consumed: 299.7511031627655 seconds\n"
     ]
    }
   ],
   "source": [
    "save_bottleneck_features(VGG19, (224, 224), 'VGG19', vgg19.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "Found 7390 images belonging to 2 classes.\n",
      "ResNet50 extrac features total consumed: 251.73025679588318 seconds\n"
     ]
    }
   ],
   "source": [
    "save_bottleneck_features(ResNet50, (224, 224), 'ResNet50', resnet50.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "Found 7390 images belonging to 2 classes.\n",
      "InceptionV3 extrac features total consumed: 322.9861743450165 seconds\n"
     ]
    }
   ],
   "source": [
    "save_bottleneck_features(InceptionV3, (299, 299), 'InceptionV3', inception_v3.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "Found 7390 images belonging to 2 classes.\n",
      "Xception extrac features total consumed: 494.8071129322052 seconds\n"
     ]
    }
   ],
   "source": [
    "save_bottleneck_features(Xception, (299, 299), 'Xception', xception.preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 迁移学习\n",
    "- 至此针对以上模型的深度特征提取完毕\n",
    "- VGG16耗时约4分19秒，VGG19耗时越5分3秒，ResNet50耗时约4分14秒，InceptionV3耗时约5分45秒，Xception耗时约8分钟19秒\n",
    "- 依据这些深度特征我们可以来构建新的模型并且只需要构建最后一层\n",
    "- 这里首先构建一个dropout层，参数为0.5，最后构建一个全连接层来做分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.image import *\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "def retrieve_features(files):\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    X_local_test = []\n",
    "    y_local_test = []\n",
    "    \n",
    "    for filename in files:\n",
    "        with h5py.File(filename, 'r') as h:\n",
    "            X_train.append(np.array(h['train']))\n",
    "            X_test.append(np.array(h['test']))\n",
    "            X_local_test.append(np.array(h['local_test']))\n",
    "            y_train = np.array(h['label'])\n",
    "            y_local_test = np.array(h['local_test_label'])\n",
    "        \n",
    "    X_train = np.concatenate(X_train, axis=1)\n",
    "    X_test = np.concatenate(X_test, axis=1)\n",
    "    X_local_test = np.concatenate(X_local_test, axis=1)\n",
    "    \n",
    "    return X_train, X_test, X_local_test, y_train, y_local_test\n",
    "\n",
    "    \n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    # construct model\n",
    "    input_tensor = Input(X_train.shape[1:])\n",
    "    x = BatchNormalization()(input_tensor)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, x)\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # train model\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, batch_size=200, epochs=10, validation_split=0.2, verbose=1)\n",
    "    end_time = time.time()\n",
    "    print(\"Trainning model total consumed:{} seconds\".format(end_time - start_time))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(X_local_test, y_local_test, model):\n",
    "    evaluate = model.evaluate(X_local_test, y_local_test, verbose=1)\n",
    "    print(\"Evaluate result, loss:{} , acc:{}\".format(evaluate[0], evaluate[1]))\n",
    "\n",
    "\n",
    "def generate_submission_csv(X_test, model):\n",
    "\n",
    "    y_pred = model.predict(X_test, verbose=1)\n",
    "    #y_pred = y_pred.clip(min=0.005, max=1)\n",
    "\n",
    "    df = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "    gen = ImageDataGenerator()\n",
    "    test_generator = gen.flow_from_directory('data/test/', (224, 224), shuffle=False, batch_size=32, class_mode=None)\n",
    "\n",
    "    for i, fname in enumerate(test_generator.filenames):\n",
    "        index = int(fname[fname.rfind('\\\\')+1:fname.rfind('.')])\n",
    "        df.at[index-1, 'label'] = y_pred[i]\n",
    "\n",
    "\n",
    "    df.to_csv('data/pred.csv', index=None)\n",
    "    df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 66us/step - loss: 0.0438 - acc: 0.9854 - val_loss: 0.0259 - val_acc: 0.9932\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 51us/step - loss: 0.0155 - acc: 0.9953 - val_loss: 0.0196 - val_acc: 0.9944\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 0.0101 - acc: 0.9967 - val_loss: 0.0207 - val_acc: 0.9944\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 48us/step - loss: 0.0073 - acc: 0.9980 - val_loss: 0.0334 - val_acc: 0.9912\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0236 - val_acc: 0.9942\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 50us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0341 - val_acc: 0.9910\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0276 - val_acc: 0.9938\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 52us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0254 - val_acc: 0.9946\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 50us/step - loss: 0.0024 - acc: 0.9996 - val_loss: 0.0269 - val_acc: 0.9940\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.0319 - val_acc: 0.9928\n",
      "Trainning model total consumed:10.235456943511963 seconds\n",
      "7390/7390 [==============================] - 1s 97us/step\n",
      "Evaluate result, loss:0.015419765178416724 , acc:0.9952638700947226\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate using all the models bottleneck_features\n",
    "bottleneck_files = [\"bottleneck_features/VGG16_bottleneck_features.h5\",\n",
    "                    \"bottleneck_features/VGG19_bottleneck_features.h5\", \n",
    "                    \"bottleneck_features/ResNet50_bottleneck_features.h5\",\n",
    "                    \"bottleneck_features/InceptionV3_bottleneck_features.h5\",\n",
    "                    \"bottleneck_features/Xception_bottleneck_features.h5\"]\n",
    "\n",
    "\n",
    "X_train, X_test, X_local_test, y_train, y_local_test = retrieve_features(bottleneck_files)\n",
    "\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "evaluate_model(X_local_test, y_local_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 59us/step - loss: 0.1900 - acc: 0.9271 - val_loss: 0.1456 - val_acc: 0.9448\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.0791 - acc: 0.9736 - val_loss: 0.1058 - val_acc: 0.9584\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 50us/step - loss: 0.0642 - acc: 0.9777 - val_loss: 0.0902 - val_acc: 0.9632\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 0.0555 - acc: 0.9802 - val_loss: 0.0877 - val_acc: 0.9640\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.0527 - acc: 0.9808 - val_loss: 0.0873 - val_acc: 0.9646\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 54us/step - loss: 0.0473 - acc: 0.9833 - val_loss: 0.0724 - val_acc: 0.9702\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 0.0474 - acc: 0.9828 - val_loss: 0.0726 - val_acc: 0.9708\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 48us/step - loss: 0.0445 - acc: 0.9841 - val_loss: 0.0771 - val_acc: 0.9678\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 48us/step - loss: 0.0439 - acc: 0.9834 - val_loss: 0.0633 - val_acc: 0.9736\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 51us/step - loss: 0.0410 - acc: 0.9856 - val_loss: 0.0742 - val_acc: 0.9698\n",
      "Trainning model total consumed:10.407723903656006 seconds\n",
      "7390/7390 [==============================] - 1s 87us/step\n",
      "Evaluate result, loss:0.08219051772916969 , acc:0.9677943166441136\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate using VGG16\n",
    "bottleneck_files = [\"bottleneck_features/VGG16_bottleneck_features.h5\"]\n",
    "\n",
    "X_train, X_test, X_local_test, y_train, y_local_test = retrieve_features(bottleneck_files)\n",
    "\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "evaluate_model(X_local_test, y_local_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 48us/step - loss: 0.2232 - acc: 0.9101 - val_loss: 0.1712 - val_acc: 0.9312\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 40us/step - loss: 0.0907 - acc: 0.9709 - val_loss: 0.1169 - val_acc: 0.9520\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 30us/step - loss: 0.0691 - acc: 0.9769 - val_loss: 0.1008 - val_acc: 0.9588\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 47us/step - loss: 0.0605 - acc: 0.9793 - val_loss: 0.0887 - val_acc: 0.9648\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0551 - acc: 0.9810 - val_loss: 0.0814 - val_acc: 0.9672\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 39us/step - loss: 0.0506 - acc: 0.9826 - val_loss: 0.0712 - val_acc: 0.9710\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 0s 25us/step - loss: 0.0482 - acc: 0.9831 - val_loss: 0.0846 - val_acc: 0.9656\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 40us/step - loss: 0.0456 - acc: 0.9839 - val_loss: 0.0708 - val_acc: 0.9716\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 40us/step - loss: 0.0435 - acc: 0.9850 - val_loss: 0.0674 - val_acc: 0.9732\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 34us/step - loss: 0.0428 - acc: 0.9855 - val_loss: 0.0670 - val_acc: 0.9740\n",
      "Trainning model total consumed:7.750809907913208 seconds\n",
      "7390/7390 [==============================] - 1s 97us/step\n",
      "Evaluate result, loss:0.09374141918086662 , acc:0.9645466847090663\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate using VGG19\n",
    "bottleneck_files = [\"bottleneck_features/VGG19_bottleneck_features.h5\"]\n",
    "\n",
    "X_train, X_test, X_local_test, y_train, y_local_test = retrieve_features(bottleneck_files)\n",
    "\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "evaluate_model(X_local_test, y_local_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 53us/step - loss: 0.1023 - acc: 0.9607 - val_loss: 0.0591 - val_acc: 0.9786\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0389 - acc: 0.9868 - val_loss: 0.0480 - val_acc: 0.9822\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 0.0315 - acc: 0.9897 - val_loss: 0.0426 - val_acc: 0.9846\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.0266 - acc: 0.9917 - val_loss: 0.0552 - val_acc: 0.9794\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 47us/step - loss: 0.0246 - acc: 0.9917 - val_loss: 0.0429 - val_acc: 0.9852\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 49us/step - loss: 0.0215 - acc: 0.9935 - val_loss: 0.0543 - val_acc: 0.9820\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 50us/step - loss: 0.0203 - acc: 0.9931 - val_loss: 0.0570 - val_acc: 0.9818\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 54us/step - loss: 0.0183 - acc: 0.9943 - val_loss: 0.0499 - val_acc: 0.9834\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0168 - acc: 0.9944 - val_loss: 0.0640 - val_acc: 0.9788\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.0153 - acc: 0.9956 - val_loss: 0.0479 - val_acc: 0.9838\n",
      "Trainning model total consumed:9.15723466873169 seconds\n",
      "7390/7390 [==============================] - 1s 154us/step\n",
      "Evaluate result, loss:0.06548396722878 , acc:0.9783491204330176\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate using ResNet50\n",
    "bottleneck_files = [\"bottleneck_features/ResNet50_bottleneck_features.h5\"]\n",
    "\n",
    "X_train, X_test, X_local_test, y_train, y_local_test = retrieve_features(bottleneck_files)\n",
    "\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "evaluate_model(X_local_test, y_local_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 64us/step - loss: 0.0429 - acc: 0.9873 - val_loss: 0.0353 - val_acc: 0.9906\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 52us/step - loss: 0.0189 - acc: 0.9948 - val_loss: 0.0296 - val_acc: 0.9922\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 60us/step - loss: 0.0162 - acc: 0.9952 - val_loss: 0.0386 - val_acc: 0.9902\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 63us/step - loss: 0.0122 - acc: 0.9965 - val_loss: 0.0297 - val_acc: 0.9924\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 67us/step - loss: 0.0105 - acc: 0.9966 - val_loss: 0.0307 - val_acc: 0.9920\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 61us/step - loss: 0.0088 - acc: 0.9977 - val_loss: 0.0289 - val_acc: 0.9928\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 63us/step - loss: 0.0076 - acc: 0.9974 - val_loss: 0.0329 - val_acc: 0.9916\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 0.0075 - acc: 0.9981 - val_loss: 0.0248 - val_acc: 0.9934\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 66us/step - loss: 0.0066 - acc: 0.9981 - val_loss: 0.0312 - val_acc: 0.9922\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.0466 - val_acc: 0.9882\n",
      "Trainning model total consumed:12.313801765441895 seconds\n",
      "7390/7390 [==============================] - 0s 53us/step\n",
      "Evaluate result, loss:0.020657120406509036 , acc:0.9928281461434371\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate using Xception\n",
    "bottleneck_files = [\"bottleneck_features/Xception_bottleneck_features.h5\"]\n",
    "\n",
    "X_train, X_test, X_local_test, y_train, y_local_test = retrieve_features(bottleneck_files)\n",
    "\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "evaluate_model(X_local_test, y_local_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 73us/step - loss: 0.0521 - acc: 0.9809 - val_loss: 0.0220 - val_acc: 0.9930\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 0.0198 - acc: 0.9938 - val_loss: 0.0235 - val_acc: 0.9928\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 60us/step - loss: 0.0168 - acc: 0.9947 - val_loss: 0.0351 - val_acc: 0.9900\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 0.0133 - acc: 0.9957 - val_loss: 0.0269 - val_acc: 0.9916\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 58us/step - loss: 0.0122 - acc: 0.9954 - val_loss: 0.0266 - val_acc: 0.9930\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 60us/step - loss: 0.0118 - acc: 0.9962 - val_loss: 0.0340 - val_acc: 0.9906\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 52us/step - loss: 0.0100 - acc: 0.9968 - val_loss: 0.0268 - val_acc: 0.9930\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 52us/step - loss: 0.0088 - acc: 0.9969 - val_loss: 0.0420 - val_acc: 0.9890\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.0075 - acc: 0.9976 - val_loss: 0.0341 - val_acc: 0.9912\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 57us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 0.0369 - val_acc: 0.9910\n",
      "Trainning model total consumed:11.642153978347778 seconds\n",
      "7390/7390 [==============================] - 1s 125us/step\n",
      "Evaluate result, loss:0.023863461442611998 , acc:0.993234100135318\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate using Inception\n",
    "bottleneck_files = [\"bottleneck_features/InceptionV3_bottleneck_features.h5\"]\n",
    "\n",
    "X_train, X_test, X_local_test, y_train, y_local_test = retrieve_features(bottleneck_files)\n",
    "\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "evaluate_model(X_local_test, y_local_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 68us/step - loss: 0.0401 - acc: 0.9861 - val_loss: 0.0217 - val_acc: 0.9932\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 49us/step - loss: 0.0152 - acc: 0.9955 - val_loss: 0.0203 - val_acc: 0.9938\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 0.0100 - acc: 0.9971 - val_loss: 0.0175 - val_acc: 0.9938\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 56us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0272 - val_acc: 0.9924\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 49us/step - loss: 0.0054 - acc: 0.9987 - val_loss: 0.0265 - val_acc: 0.9926\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 48us/step - loss: 0.0038 - acc: 0.9993 - val_loss: 0.0280 - val_acc: 0.9922\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 51us/step - loss: 0.0031 - acc: 0.9992 - val_loss: 0.0226 - val_acc: 0.9948\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 48us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0304 - val_acc: 0.9928\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 50us/step - loss: 0.0021 - acc: 0.9997 - val_loss: 0.0283 - val_acc: 0.9928\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 62us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0289 - val_acc: 0.9930\n",
      "Trainning model total consumed:10.891825914382935 seconds\n",
      "7390/7390 [==============================] - 1s 137us/step\n",
      "Evaluate result, loss:0.01649118572826267 , acc:0.9939106901217862\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate using all the models bottleneck_features\n",
    "bottleneck_files = [\"bottleneck_features/InceptionV3_bottleneck_features.h5\",\n",
    "                    \"bottleneck_features/Xception_bottleneck_features.h5\"]\n",
    "\n",
    "\n",
    "X_train, X_test, X_local_test, y_train, y_local_test = retrieve_features(bottleneck_files)\n",
    "\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "evaluate_model(X_local_test, y_local_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 0s 34us/step\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "generate_submission_csv(X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.994 0.005\n"
     ]
    }
   ],
   "source": [
    "def logloss(true_label, predicted, eps=1e-15):\n",
    "  p = np.clip(predicted, eps, 1 - eps)\n",
    "  if true_label == 1:\n",
    "    return -log(p)\n",
    "  else:\n",
    "    return -log(1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.298317366548036\n",
      "-46.051701859880914\n"
     ]
    }
   ],
   "source": [
    "print(math.log(0.005))\n",
    "print(math.log(0.00000000000000000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
